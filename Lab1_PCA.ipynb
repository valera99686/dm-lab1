{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Лабораторна робота №1: Зменшення розмірності даних (PCA + MNIST)\n\n**Мета:** Ознайомитися з основами зменшення розмірності даних на прикладі зображень рукописних цифр із набору MNIST. Навчитись виконувати метод головних компонент (PCA), інтерпретувати результати, оцінювати втрату інформації після проєкції та реконструкції зображень.\n\n**Бібліотеки:** numpy, pandas, matplotlib, scikit-learn (PCA)\n\n---\n\n## 1. Завантаження та підготовка даних"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Імпорт бібліотеки NumPy для роботи з масивами та математичними операціями\nimport numpy as np\n\n# Імпорт бібліотеки Pandas для роботи з табличними даними\nimport pandas as pd\n\n# Імпорт модуля matplotlib.pyplot для побудови графіків та візуалізацій\nimport matplotlib.pyplot as plt\n\n# Імпорт функції fetch_openml для завантаження датасетів з репозиторію OpenML\nfrom sklearn.datasets import fetch_openml\n\n# Імпорт класу PCA (Principal Component Analysis) для зменшення розмірності\nfrom sklearn.decomposition import PCA\n\n# Завантаження датасету MNIST (рукописні цифри 0-9)\n# mnist_784 — 70000 зображень 28x28 пікселів, розгорнутих у вектори довжиною 784\n# return_X_y=True повертає окремо матрицю ознак X та вектор міток y\n# as_frame=False повертає дані як numpy-масиви (а не pandas DataFrame)\nX, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)\n\n# Перетворення міток з рядків ('0','1',...,'9') у цілі числа (0,1,...,9)\ny_int = y.astype(int)\n\n# Вивід розмірності матриці X: (кількість зображень, кількість пікселів)\nprint(f\"Розмірність матриці X: {X.shape}\")\n\n# Вивід кількості різних цифр у датасеті\nprint(f\"Кількість унікальних міток: {len(np.unique(y_int))}\")\n\n# Вивід самих міток (від 0 до 9)\nprint(f\"Унікальні мітки: {np.unique(y_int)}\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- Візуалізація 10 випадкових зображень цифр ---\n\n# Встановлюємо seed для відтворюваності (щоб кожен запуск давав ті самі \"випадкові\" індекси)\nnp.random.seed(42)\n\n# Обираємо 10 випадкових індексів із діапазону [0, 70000), без повторень\nrandom_indices = np.random.choice(len(X), size=10, replace=False)\n\n# Створюємо фігуру з 10 підграфіками в один рядок, розмір 15x2 дюймів\nfig, axes = plt.subplots(1, 10, figsize=(15, 2))\n\n# Цикл по 10 обраним зображенням\nfor i, idx in enumerate(random_indices):\n    # Відображаємо зображення: reshape з вектора (784,) у матрицю (28, 28)\n    # cmap='gray' — відтінки сірого (0=чорний, 255=білий)\n    axes[i].imshow(X[idx].reshape(28, 28), cmap='gray')\n\n    # Підпис — справжня мітка цифри\n    axes[i].set_title(f\"{y_int[idx]}\")\n\n    # Прибираємо осі координат для чистішого вигляду\n    axes[i].axis('off')\n\n# Загальний заголовок фігури\nplt.suptitle(\"10 випадкових зображень з MNIST\", fontsize=14)\n\n# Автоматичне коригування відступів між підграфіками\nplt.tight_layout()\n\n# Відображення фігури\nplt.show()",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Метод головних компонент (PCA) з 3 компонентами\n",
    "\n",
    "PCA — лінійний метод зменшення розмірності, який знаходить напрямки максимальної дисперсії даних (головні компоненти) та проєктує дані на ці напрямки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- PCA з 3 компонентами ---\n\n# Створюємо об'єкт PCA з кількістю компонент = 3\n# PCA знаходить 3 ортогональних напрямки максимальної дисперсії у 784-вимірному просторі\npca_3 = PCA(n_components=3)\n\n# fit_transform виконує два кроки одночасно:\n#   1) fit — обчислює головні компоненти (власні вектори коваріаційної матриці)\n#   2) transform — проєктує всі 70000 зображень на ці 3 компоненти\n# Результат: матриця (70000, 3) — кожне зображення описується 3 числами замість 784\nX_pca = pca_3.fit_transform(X)\n\n# Вивід нової розмірності даних після PCA\nprint(f\"Розмірність після PCA: {X_pca.shape}\")\n\n# Вивід відсотку дисперсії, який пояснює кожна з 3 компонент\n# explained_variance_ratio_ — масив часток дисперсії для кожної компоненти\nprint(f\"\\nВідсоток поясненої дисперсії для кожної компоненти:\")\nfor i, ratio in enumerate(pca_3.explained_variance_ratio_):\n    print(f\"  PC{i+1}: {ratio:.4f} ({ratio*100:.2f}%)\")\n\n# Сума дисперсій всіх 3 компонент — показує скільки інформації збережено\nprint(f\"\\nСумарна пояснена дисперсія: {sum(pca_3.explained_variance_ratio_)*100:.2f}%\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 3D візуалізація у просторі перших трьох компонент"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# --- 3D візуалізація PCA-проєкції ---\n\n# Створюємо фігуру розміром 12x9 дюймів\nfig = plt.figure(figsize=(12, 9))\n\n# Додаємо 3D-підграфік (projection='3d' вмикає тривимірний режим)\nax = fig.add_subplot(111, projection='3d')\n\n# Встановлюємо seed для відтворюваності\nnp.random.seed(42)\n\n# Обираємо підмножину з 5000 точок для швидшої візуалізації\n# (70000 точок на 3D-графіку — занадто повільно та захаращено)\nsample_idx = np.random.choice(len(X_pca), size=5000, replace=False)\n\n# Будуємо 3D scatter plot (точковий графік):\n#   X_pca[sample_idx, 0] — значення першої головної компоненти (PC1)\n#   X_pca[sample_idx, 1] — значення другої головної компоненти (PC2)\n#   X_pca[sample_idx, 2] — значення третьої головної компоненти (PC3)\n#   c=y_int[sample_idx] — колір кожної точки відповідає мітці цифри (0-9)\n#   cmap='tab10' — палітра з 10 різних кольорів для 10 класів\n#   alpha=0.5 — напівпрозорість точок (щоб видно перекриття)\n#   s=5 — розмір кожної точки\nscatter = ax.scatter(\n    X_pca[sample_idx, 0],\n    X_pca[sample_idx, 1],\n    X_pca[sample_idx, 2],\n    c=y_int[sample_idx],\n    cmap='tab10',\n    alpha=0.5,\n    s=5\n)\n\n# Підписи осей — кожна вісь відповідає одній головній компоненті\nax.set_xlabel('PC1')\nax.set_ylabel('PC2')\nax.set_zlabel('PC3')\n\n# Заголовок графіку\nax.set_title('3D візуалізація MNIST після PCA (3 компоненти)')\n\n# Кольорова шкала збоку — показує відповідність кольору до цифри\nplt.colorbar(scatter, label='Цифра', shrink=0.6)\n\n# Автоматичне коригування відступів\nplt.tight_layout()\n\n# Відображення графіку\nplt.show()\n\n# Текстовий аналіз результатів візуалізації\nprint(\"\"\"\nАналіз кластерів:\n- Деякі цифри (наприклад, 0, 1) формують відносно чіткі кластери у 3D просторі.\n- Інші цифри (наприклад, 4 і 9, 3 і 5) значно перекриваються,\n  що свідчить про їхню візуальну схожість.\n- 3 компоненти пояснюють лише ~23% дисперсії, тому повне розділення класів неможливе.\n- Для кращого розділення потрібно більше компонент.\n\"\"\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Реконструкція зображень після зменшення розмірності\n\nЗворотне перетворення (inverse_transform) дозволяє відновити зображення з PCA-простору назад у 784-вимірний простір. Порівняння оригіналу з реконструкцією показує, скільки інформації втрачається."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Реконструкція зображень з 3 компонент ---\n\n# inverse_transform перетворює дані з PCA-простору (70000, 3) назад у початковий (70000, 784)\n# Це \"наближена\" реконструкція — частина інформації втрачена при стисканні до 3 компонент\nX_reconstructed = pca_3.inverse_transform(X_pca)\n\n# Встановлюємо seed для вибору тих самих 10 зображень\nnp.random.seed(42)\n\n# Обираємо 10 випадкових індексів для порівняння\nindices = np.random.choice(len(X), size=10, replace=False)\n\n# Створюємо фігуру з двома рядками по 10 зображень: верхній — оригінал, нижній — реконструкція\nfig, axes = plt.subplots(2, 10, figsize=(18, 4))\n\n# Цикл по 10 обраним зображенням\nfor i, idx in enumerate(indices):\n    # Верхній рядок: оригінальне зображення 28x28\n    axes[0, i].imshow(X[idx].reshape(28, 28), cmap='gray')\n    axes[0, i].set_title(f\"{y_int[idx]}\", fontsize=10)  # Мітка цифри як заголовок\n    axes[0, i].axis('off')  # Без осей\n\n    # Нижній рядок: реконструйоване зображення з 3 компонент PCA\n    axes[1, i].imshow(X_reconstructed[idx].reshape(28, 28), cmap='gray')\n    axes[1, i].axis('off')  # Без осей\n\n# Підписи рядків: \"Оригінал\" та \"PCA (k=3)\"\naxes[0, 0].set_ylabel('Оригінал', fontsize=12, rotation=0, labelpad=60)\naxes[1, 0].set_ylabel('PCA (k=3)', fontsize=12, rotation=0, labelpad=60)\n\n# Загальний заголовок фігури\nplt.suptitle('Оригінальні та реконструйовані зображення (3 компоненти PCA)', fontsize=14)\n\n# Автоматичне коригування відступів\nplt.tight_layout()\n\n# Відображення фігури\nplt.show()\n\n# Обчислення MSE (Mean Squared Error) — середньоквадратична помилка реконструкції\n# MSE = середнє значення (оригінал - реконструкція)^2 по всіх пікселях всіх зображень\nmse_3 = np.mean((X - X_reconstructed) ** 2)\nprint(f\"MSE реконструкції з 3 компонентами: {mse_3:.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Аналіз залежності MSE та поясненої дисперсії від кількості компонент"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# --- Аналіз залежності MSE та поясненої дисперсії від кількості компонент k ---\n\n# Список значень k (кількість компонент PCA) для дослідження\n# Від 1 (мінімальне стиснення) до 500 (майже без втрат, бо 784 — максимум)\nk_values = [1, 2, 3, 5, 10, 20, 50, 100, 150, 200, 300, 500]\n\n# Списки для збереження результатів MSE та поясненої дисперсії\nmse_values = []\nexplained_variance_values = []\n\n# Цикл: для кожного значення k виконуємо PCA та обчислюємо метрики\nfor k in k_values:\n    # Створюємо PCA з k компонентами\n    pca_k = PCA(n_components=k)\n\n    # Навчаємо PCA та проєктуємо дані у k-вимірний простір\n    X_pca_k = pca_k.fit_transform(X)\n\n    # Зворотне перетворення: з k-вимірного простору назад у 784-вимірний\n    X_reconstructed_k = pca_k.inverse_transform(X_pca_k)\n\n    # Обчислюємо MSE між оригіналом та реконструкцією\n    # Чим менше MSE — тим краща реконструкція\n    mse_k = np.mean((X - X_reconstructed_k) ** 2)\n    mse_values.append(mse_k)\n\n    # Сумарна пояснена дисперсія для k компонент\n    # Показує, яку частку загальної варіативності даних пояснюють k компонент\n    total_var = np.sum(pca_k.explained_variance_ratio_)\n    explained_variance_values.append(total_var)\n\n    # Вивід результатів для кожного k\n    print(f\"k={k:>3d}: MSE={mse_k:>8.2f}, Пояснена дисперсія={total_var*100:>6.2f}%\")\n\n# --- Побудова двох графіків поряд ---\n\n# Створюємо фігуру з двома підграфіками: лівий — MSE(k), правий — дисперсія(k)\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n\n# --- Лівий графік: MSE(k) ---\n# 'bo-' — сині точки з'єднані лініями\nax1.plot(k_values, mse_values, 'bo-', linewidth=2, markersize=6)\nax1.set_xlabel('Кількість компонент (k)', fontsize=12)  # Підпис осі X\nax1.set_ylabel('MSE', fontsize=12)  # Підпис осі Y\nax1.set_title('Залежність MSE від кількості компонент', fontsize=13)  # Заголовок\nax1.grid(True, alpha=0.3)  # Сітка з прозорістю 0.3\nax1.set_xscale('log')  # Логарифмічна шкала по X (бо k змінюється від 1 до 500)\n\n# --- Правий графік: Пояснена дисперсія(k) ---\n# 'ro-' — червоні точки з'єднані лініями\nax2.plot(k_values, [v * 100 for v in explained_variance_values], 'ro-', linewidth=2, markersize=6)\nax2.set_xlabel('Кількість компонент (k)', fontsize=12)  # Підпис осі X\nax2.set_ylabel('Пояснена дисперсія (%)', fontsize=12)  # Підпис осі Y\nax2.set_title('Залежність поясненої дисперсії від кількості компонент', fontsize=13)  # Заголовок\nax2.grid(True, alpha=0.3)  # Сітка\nax2.set_xscale('log')  # Логарифмічна шкала по X\n\n# Горизонтальна пунктирна лінія на рівні 95% — порогове значення \"достатньої\" дисперсії\nax2.axhline(y=95, color='green', linestyle='--', alpha=0.7, label='95% дисперсії')\n\n# Легенда з поясненням пунктирної лінії\nax2.legend()\n\n# Автоматичне коригування відступів\nplt.tight_layout()\n\n# Відображення графіків\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Висновки\n",
    "\n",
    "1. **PCA** успішно зменшує розмірність даних із 784 до довільної кількості компонент.\n",
    "2. З **3 компонентами** (~23% дисперсії) форми цифр ще розпізнаються, але зображення розмиті.\n",
    "3. **3D візуалізація** показує часткове розділення класів — цифри 0 та 1 формують окремі кластери, тоді як 4/9, 3/5 перекриваються через візуальну схожість.\n",
    "4. **MSE** швидко зменшується для перших ~50 компонент, далі покращення сповільнюється.\n",
    "5. Для досягнення **95% поясненої дисперсії** потрібно приблизно 150–200 компонент, що значно менше за початкові 784 ознаки."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}